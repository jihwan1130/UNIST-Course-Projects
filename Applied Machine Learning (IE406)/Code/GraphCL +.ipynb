{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GraphCL - Unsupervised "
      ],
      "metadata": {
        "id": "ikGDH_xL9QxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layers"
      ],
      "metadata": {
        "id": "qkWRirp4VZBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, n_h):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.f_k = nn.Bilinear(n_h, n_h, 1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Bilinear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n",
        "        c_x = torch.unsqueeze(c, 1)\n",
        "        c_x = c_x.expand_as(h_pl)\n",
        "\n",
        "        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 2)\n",
        "        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 2)\n",
        "\n",
        "        if s_bias1 is not None:\n",
        "            sc_1 += s_bias1\n",
        "        if s_bias2 is not None:\n",
        "            sc_2 += s_bias2\n",
        "\n",
        "        logits = torch.cat((sc_1, sc_2), 1)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class Discriminator2(nn.Module):\n",
        "    def __init__(self, n_h):\n",
        "        super(Discriminator2, self).__init__()\n",
        "        self.f_k = nn.Bilinear(n_h, n_h, 1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Bilinear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n",
        "        # c_x = torch.unsqueeze(c, 1)\n",
        "        # c_x = c_x.expand_as(h_pl)\n",
        "        c_x = c\n",
        "        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 2) #positive\n",
        "        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 2) #negative\n",
        "\n",
        "        if s_bias1 is not None:\n",
        "            sc_1 += s_bias1\n",
        "        if s_bias2 is not None:\n",
        "            sc_2 += s_bias2\n",
        "\n",
        "        logits = torch.cat((sc_1, sc_2), 1)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_ft, out_ft, act, bias=True):\n",
        "        super(GCN, self).__init__()\n",
        "        self.fc = nn.Linear(in_ft, out_ft, bias=False)\n",
        "        self.act = nn.PReLU() if act == 'prelu' else act\n",
        "        \n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.FloatTensor(out_ft))\n",
        "            self.bias.data.fill_(0.0)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    # Shape of seq: (batch, nodes, features)\n",
        "    def forward(self, seq, adj, sparse=False):\n",
        "        seq_fts = self.fc(seq)\n",
        "        if sparse:\n",
        "            out = torch.unsqueeze(torch.spmm(adj, torch.squeeze(seq_fts, 0)), 0)\n",
        "        else:\n",
        "            out = torch.bmm(adj, seq_fts)\n",
        "        if self.bias is not None:\n",
        "            out += self.bias\n",
        "        \n",
        "        return self.act(out)\n",
        "\n",
        "# Applies an average on seq, of shape (batch, nodes, features)\n",
        "# While taking into account the masking of msk\n",
        "class AvgReadout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AvgReadout, self).__init__()\n",
        "\n",
        "    def forward(self, seq, msk):\n",
        "        if msk is None:\n",
        "            return torch.mean(seq, 1)\n",
        "        else:\n",
        "            msk = torch.unsqueeze(msk, -1)\n",
        "            return torch.sum(seq * msk, 1) / torch.sum(msk)"
      ],
      "metadata": {
        "id": "pApvt-NFVpS-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "hEheV1LZT6Zj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WKduzMFCSy72"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "#from layers import GCN, AvgReadout, Discriminator, Discriminator2\n",
        "import pdb\n",
        "\n",
        "class DGI(nn.Module):\n",
        "    def __init__(self, n_in, n_h, activation):\n",
        "        super(DGI, self).__init__()\n",
        "        self.gcn = GCN(n_in, n_h, activation)\n",
        "        self.read = AvgReadout()\n",
        "        self.sigm = nn.Sigmoid()\n",
        "        self.disc = Discriminator(n_h)\n",
        "        self.disc2 = Discriminator2(n_h)\n",
        "\n",
        "    def forward(self, seq1, seq2, seq3, seq4, adj, aug_adj1, aug_adj2, sparse, msk, samp_bias1, samp_bias2, aug_type):\n",
        "        \n",
        "        h_0 = self.gcn(seq1, adj, sparse) #positive sample\n",
        "        if aug_type == 'edge':\n",
        "\n",
        "            h_1 = self.gcn(seq1, aug_adj1, sparse)\n",
        "            h_3 = self.gcn(seq1, aug_adj2, sparse)\n",
        "\n",
        "        elif aug_type == 'mask':\n",
        "\n",
        "            h_1 = self.gcn(seq3, adj, sparse)\n",
        "            h_3 = self.gcn(seq4, adj, sparse)\n",
        "\n",
        "        elif aug_type == 'node' or aug_type == 'subgraph':\n",
        "\n",
        "            h_1 = self.gcn(seq3, aug_adj1, sparse)\n",
        "            h_3 = self.gcn(seq4, aug_adj2, sparse)\n",
        "            \n",
        "        else:\n",
        "            assert False\n",
        "            \n",
        "        c_1 = self.read(h_1, msk)\n",
        "        c_1= self.sigm(c_1)\n",
        "\n",
        "        c_3 = self.read(h_3, msk)\n",
        "        c_3= self.sigm(c_3)\n",
        "\n",
        "        h_2 = self.gcn(seq2, adj, sparse) #negative sample\n",
        "\n",
        "        ret1 = self.disc(c_1, h_0, h_2, samp_bias1, samp_bias2) #logit (x1, x2)   #positive sample logit -> 1\n",
        "        ret2 = self.disc(c_3, h_0, h_2, samp_bias1, samp_bias2) #logit (x1', x2') #negative sample logit -> 0\n",
        "\n",
        "        ret = ret1 + ret2\n",
        "        return ret\n",
        "\n",
        "    # Detach the return variables\n",
        "    def embed(self, seq, adj, sparse, msk):\n",
        "        h_1 = self.gcn(seq, adj, sparse)\n",
        "        c = self.read(h_1, msk)\n",
        "\n",
        "        return h_1.detach(), c.detach()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class LogReg(nn.Module):\n",
        "    def __init__(self, ft_in, nb_classes):\n",
        "        super(LogReg, self).__init__()\n",
        "        self.fc = nn.Linear(ft_in, nb_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, seq):\n",
        "        ret = self.fc(seq)\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "8KBBSanlXL5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class process():\n",
        "    def parse_skipgram(fname):\n",
        "        with open(fname) as f:\n",
        "            toks = list(f.read().split())\n",
        "        nb_nodes = int(toks[0])\n",
        "        nb_features = int(toks[1])\n",
        "        ret = np.empty((nb_nodes, nb_features))\n",
        "        it = 2\n",
        "        for i in range(nb_nodes):\n",
        "            cur_nd = int(toks[it]) - 1\n",
        "            it += 1\n",
        "            for j in range(nb_features):\n",
        "                cur_ft = float(toks[it])\n",
        "                ret[cur_nd][j] = cur_ft\n",
        "                it += 1\n",
        "        return ret\n",
        "\n",
        "    # Process a (subset of) a TU dataset into standard form\n",
        "    def process_tu(data, nb_nodes):\n",
        "        nb_graphs = len(data)\n",
        "        ft_size = data.num_features\n",
        "\n",
        "        features = np.zeros((nb_graphs, nb_nodes, ft_size))\n",
        "        adjacency = np.zeros((nb_graphs, nb_nodes, nb_nodes))\n",
        "        labels = np.zeros(nb_graphs)\n",
        "        sizes = np.zeros(nb_graphs, dtype=np.int32)\n",
        "        masks = np.zeros((nb_graphs, nb_nodes))\n",
        "        \n",
        "        for g in range(nb_graphs):\n",
        "            sizes[g] = data[g].x.shape[0]\n",
        "            features[g, :sizes[g]] = data[g].x\n",
        "            labels[g] = data[g].y[0]\n",
        "            masks[g, :sizes[g]] = 1.0\n",
        "            e_ind = data[g].edge_index\n",
        "            coo = sp.coo_matrix((np.ones(e_ind.shape[1]), (e_ind[0, :], e_ind[1, :])), shape=(nb_nodes, nb_nodes))\n",
        "            adjacency[g] = coo.todense()\n",
        "\n",
        "        return features, adjacency, labels, sizes, masks\n",
        "\n",
        "    def micro_f1(logits, labels):\n",
        "        # Compute predictions\n",
        "        preds = torch.round(nn.Sigmoid()(logits))\n",
        "        \n",
        "        # Cast to avoid trouble\n",
        "        preds = preds.long()\n",
        "        labels = labels.long()\n",
        "\n",
        "        # Count true positives, true negatives, false positives, false negatives\n",
        "        tp = torch.nonzero(preds * labels).shape[0] * 1.0\n",
        "        tn = torch.nonzero((preds - 1) * (labels - 1)).shape[0] * 1.0\n",
        "        fp = torch.nonzero(preds * (labels - 1)).shape[0] * 1.0\n",
        "        fn = torch.nonzero((preds - 1) * labels).shape[0] * 1.0\n",
        "\n",
        "        # Compute micro-f1 score\n",
        "        prec = tp / (tp + fp)\n",
        "        rec = tp / (tp + fn)\n",
        "        f1 = (2 * prec * rec) / (prec + rec)\n",
        "        return f1\n",
        "\n",
        "    \"\"\"\n",
        "    Prepare adjacency matrix by expanding up to a given neighbourhood.\n",
        "    This will insert loops on every node.\n",
        "    Finally, the matrix is converted to bias vectors.\n",
        "    Expected shape: [graph, nodes, nodes]\n",
        "    \"\"\"\n",
        "    def adj_to_bias(adj, sizes, nhood=1):\n",
        "        nb_graphs = adj.shape[0]\n",
        "        mt = np.empty(adj.shape)\n",
        "        for g in range(nb_graphs):\n",
        "            mt[g] = np.eye(adj.shape[1])\n",
        "            for _ in range(nhood):\n",
        "                mt[g] = np.matmul(mt[g], (adj[g] + np.eye(adj.shape[1])))\n",
        "            for i in range(sizes[g]):\n",
        "                for j in range(sizes[g]):\n",
        "                    if mt[g][i][j] > 0.0:\n",
        "                        mt[g][i][j] = 1.0\n",
        "        return -1e9 * (1.0 - mt)\n",
        "\n",
        "\n",
        "    ###############################################\n",
        "    # This section of code adapted from tkipf/gcn #\n",
        "    ###############################################\n",
        "\n",
        "    def parse_index_file(filename):\n",
        "        \"\"\"Parse index file.\"\"\"\n",
        "        index = []\n",
        "        for line in open(filename):\n",
        "            index.append(int(line.strip()))\n",
        "        return index\n",
        "\n",
        "    def sample_mask(idx, l):\n",
        "        \"\"\"Create mask.\"\"\"\n",
        "        mask = np.zeros(l)\n",
        "        mask[idx] = 1\n",
        "        return np.array(mask, dtype=np.bool)\n",
        "\n",
        "    def load_data(dataset_str): # {'pubmed', 'citeseer', 'cora'}\n",
        "        \"\"\"Load data.\"\"\"\n",
        "        names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
        "        objects = []\n",
        "        for i in range(len(names)):\n",
        "            with open(\"/content/data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
        "                if sys.version_info > (3, 0):\n",
        "                    objects.append(pkl.load(f, encoding='latin1'))\n",
        "                else:\n",
        "                    objects.append(pkl.load(f))\n",
        "\n",
        "        x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
        "        test_idx_reorder = process.parse_index_file(\"/content/data/ind.{}.test.index.txt\".format(dataset_str))\n",
        "        test_idx_range = np.sort(test_idx_reorder)\n",
        "\n",
        "        if dataset_str == 'citeseer':\n",
        "            # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
        "            # Find isolated nodes, add them as zero-vecs into the right position\n",
        "            test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
        "            tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
        "            tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
        "            tx = tx_extended\n",
        "            ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
        "            ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
        "            ty = ty_extended\n",
        "\n",
        "        features = sp.vstack((allx, tx)).tolil()\n",
        "        features[test_idx_reorder, :] = features[test_idx_range, :]\n",
        "        adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "\n",
        "        labels = np.vstack((ally, ty))\n",
        "        labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
        "\n",
        "        idx_test = test_idx_range.tolist()\n",
        "        idx_train = range(len(y))\n",
        "        idx_val = range(len(y), len(y)+500)\n",
        "\n",
        "        return adj, features, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "    def sparse_to_tuple(sparse_mx, insert_batch=True):\n",
        "        \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "        \"\"\"Set insert_batch=True if you want to insert a batch dimension.\"\"\"\n",
        "        def to_tuple(mx):\n",
        "            if not sp.isspmatrix_coo(mx):\n",
        "                mx = mx.tocoo()\n",
        "            if insert_batch:\n",
        "                coords = np.vstack((np.zeros(mx.row.shape[0]), mx.row, mx.col)).transpose()\n",
        "                values = mx.data\n",
        "                shape = (1,) + mx.shape\n",
        "            else:\n",
        "                coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "                values = mx.data\n",
        "                shape = mx.shape\n",
        "            return coords, values, shape\n",
        "\n",
        "        if isinstance(sparse_mx, list):\n",
        "            for i in range(len(sparse_mx)):\n",
        "                sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "        else:\n",
        "            sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "        return sparse_mx\n",
        "\n",
        "    def standardize_data(f, train_mask):\n",
        "        \"\"\"Standardize feature matrix and convert to tuple representation\"\"\"\n",
        "        # standardize data\n",
        "        f = f.todense()\n",
        "        mu = f[train_mask == True, :].mean(axis=0)\n",
        "        sigma = f[train_mask == True, :].std(axis=0)\n",
        "        f = f[:, np.squeeze(np.array(sigma > 0))]\n",
        "        mu = f[train_mask == True, :].mean(axis=0)\n",
        "        sigma = f[train_mask == True, :].std(axis=0)\n",
        "        f = (f - mu) / sigma\n",
        "        return f\n",
        "\n",
        "    def preprocess_features(features):\n",
        "        \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "        rowsum = np.array(features.sum(1))\n",
        "        r_inv = np.power(rowsum, -1).flatten()\n",
        "        r_inv[np.isinf(r_inv)] = 0.\n",
        "        r_mat_inv = sp.diags(r_inv)\n",
        "        features = r_mat_inv.dot(features)\n",
        "        return features.todense(), process.sparse_to_tuple(features)\n",
        "\n",
        "    def normalize_adj(adj):\n",
        "        \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "        adj = sp.coo_matrix(adj)\n",
        "        rowsum = np.array(adj.sum(1))\n",
        "        d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "        d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "        return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "\n",
        "    def preprocess_adj(adj):\n",
        "        \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
        "        adj_normalized = process.normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "        return process.sparse_to_tuple(adj_normalized)\n",
        "\n",
        "    def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "        \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "        sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "        indices = torch.from_numpy(\n",
        "            np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "        values = torch.from_numpy(sparse_mx.data)\n",
        "        shape = torch.Size(sparse_mx.shape)\n",
        "        return torch.sparse.FloatTensor(indices, values, shape)"
      ],
      "metadata": {
        "id": "33n4dvHBXSlq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augmentation"
      ],
      "metadata": {
        "id": "8sHzXo2z7Irp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import copy\n",
        "import random\n",
        "import pdb\n",
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "\n",
        "def main():\n",
        "    pass\n",
        "\n",
        "\n",
        "def aug_random_mask(input_feature, drop_percent=0.2):\n",
        "    \n",
        "    node_num = input_feature.shape[1]\n",
        "    mask_num = int(node_num * drop_percent)\n",
        "    node_idx = [i for i in range(node_num)]\n",
        "    mask_idx = random.sample(node_idx, mask_num)\n",
        "    aug_feature = copy.deepcopy(input_feature)\n",
        "    zeros = torch.zeros_like(aug_feature[0][0])\n",
        "    for j in mask_idx:\n",
        "        aug_feature[0][j] = zeros\n",
        "    return aug_feature\n",
        "\n",
        "\n",
        "def aug_random_edge(input_adj, drop_percent=0.2):\n",
        "\n",
        "    percent = drop_percent / 2\n",
        "    row_idx, col_idx = input_adj.nonzero()\n",
        "\n",
        "    index_list = []\n",
        "    for i in range(len(row_idx)):\n",
        "        index_list.append((row_idx[i], col_idx[i]))\n",
        "\n",
        "    single_index_list = []\n",
        "    for i in list(index_list):\n",
        "        single_index_list.append(i)\n",
        "        index_list.remove((i[1], i[0]))\n",
        "    \n",
        "    \n",
        "    edge_num = int(len(row_idx) / 2)      # 9228 / 2\n",
        "    add_drop_num = int(edge_num * percent / 2) \n",
        "    aug_adj = copy.deepcopy(input_adj.todense().tolist())\n",
        "\n",
        "    edge_idx = [i for i in range(edge_num)]\n",
        "    drop_idx = random.sample(edge_idx, add_drop_num)\n",
        "\n",
        "    \n",
        "    for i in drop_idx:\n",
        "        aug_adj[single_index_list[i][0]][single_index_list[i][1]] = 0\n",
        "        aug_adj[single_index_list[i][1]][single_index_list[i][0]] = 0\n",
        "    \n",
        "    '''\n",
        "    above finish drop edges\n",
        "    '''\n",
        "    node_num = input_adj.shape[0]\n",
        "    l = [(i, j) for i in range(node_num) for j in range(i)]\n",
        "    add_list = random.sample(l, add_drop_num)\n",
        "\n",
        "    for i in add_list:\n",
        "        \n",
        "        aug_adj[i[0]][i[1]] = 1\n",
        "        aug_adj[i[1]][i[0]] = 1\n",
        "    \n",
        "    aug_adj = np.matrix(aug_adj)\n",
        "    aug_adj = sp.csr_matrix(aug_adj)\n",
        "    return aug_adj\n",
        "\n",
        "\n",
        "def aug_drop_node(input_fea, input_adj, drop_percent=0.2):\n",
        "\n",
        "    input_adj = torch.tensor(input_adj.todense().tolist())\n",
        "    input_fea = input_fea.squeeze(0)\n",
        "\n",
        "    node_num = input_fea.shape[0]\n",
        "    drop_num = int(node_num * drop_percent)    # number of drop nodes\n",
        "    all_node_list = [i for i in range(node_num)]\n",
        "\n",
        "    drop_node_list = sorted(random.sample(all_node_list, drop_num))\n",
        "\n",
        "    aug_input_fea = delete_row_col(input_fea, drop_node_list, only_row=True)\n",
        "    aug_input_adj = delete_row_col(input_adj, drop_node_list)\n",
        "\n",
        "    aug_input_fea = aug_input_fea.unsqueeze(0)\n",
        "    aug_input_adj = sp.csr_matrix(np.matrix(aug_input_adj))\n",
        "\n",
        "    return aug_input_fea, aug_input_adj\n",
        "\n",
        "#subgraph 수정\n",
        "def aug_subgraph(input_fea, input_adj, drop_percent=0.2):\n",
        "    \n",
        "    matrix_D_array = input_adj.todense()\n",
        "    matrix_D_array = np.nan_to_num(matrix_D_array) \n",
        "\n",
        "    # L 에 대한 eigenvalue, eigenvector 계산 \n",
        "    eigen_value , eigen_vector = np.linalg.eig( matrix_D_array )\n",
        "\n",
        "    # eigen value 내림차순 정렬 \n",
        "    order = np.absolute(eigen_value).argsort()[::-1] \n",
        "\n",
        "    # 정렬 순서에 따라 재정렬 \n",
        "    eigen_value = eigen_value[order] \n",
        "    eigen_vector = eigen_vector[:,order] \n",
        "\n",
        "    # 첫번째 eigen value 에 대한 eigenvector 추출 및 비중확인 \n",
        "    r = eigen_vector[:,0]  # 0번째 열 \n",
        "    value = 100*np.real(r/np.sum(r)) ## np.real : 복소수 인수의 실수부를 반환\n",
        "    print(value) #pagerank\n",
        "\n",
        "    #-------------------------------------------------------------------------------------------\n",
        "    input_adj = torch.tensor(input_adj.todense().tolist())\n",
        "    input_fea = input_fea.squeeze(0)\n",
        "    node_num = input_fea.shape[0]\n",
        "\n",
        "    all_node_list = [i for i in range(node_num)]\n",
        "    s_node_num = int(node_num * (1 - drop_percent))\n",
        "    center_node_id = value.tolist().index(max(value)) ### 수정됨\n",
        "    #center_node_id = random.randint(0, node_num - 1) ### 오리지널\n",
        "    sub_node_id_list = [center_node_id]\n",
        "    all_neighbor_list = []\n",
        "\n",
        "    for i in range(s_node_num - 1):\n",
        "        \n",
        "        all_neighbor_list += torch.nonzero(input_adj[sub_node_id_list[i]], as_tuple=False).squeeze(1).tolist()\n",
        "        \n",
        "        all_neighbor_list = list(set(all_neighbor_list))\n",
        "        new_neighbor_list = [n for n in all_neighbor_list if not n in sub_node_id_list]\n",
        "        if len(new_neighbor_list) != 0:\n",
        "            new_node = random.sample(new_neighbor_list, 1)[0]\n",
        "            sub_node_id_list.append(new_node)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    \n",
        "    drop_node_list = sorted([i for i in all_node_list if not i in sub_node_id_list])\n",
        "\n",
        "    aug_input_fea = delete_row_col(input_fea, drop_node_list, only_row=True)\n",
        "    aug_input_adj = delete_row_col(input_adj, drop_node_list)\n",
        "\n",
        "    aug_input_fea = aug_input_fea.unsqueeze(0)\n",
        "    aug_input_adj = sp.csr_matrix(np.matrix(aug_input_adj))\n",
        "\n",
        "    return aug_input_fea, aug_input_adj\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def delete_row_col(input_matrix, drop_list, only_row=False):\n",
        "\n",
        "    remain_list = [i for i in range(input_matrix.shape[0]) if i not in drop_list]\n",
        "    out = input_matrix[remain_list, :]\n",
        "    if only_row:\n",
        "        return out\n",
        "    else:\n",
        "        return out[:, remain_list]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "VqMDU-tE69QH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute"
      ],
      "metadata": {
        "id": "CRnlq-j27p5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "parser = argparse.ArgumentParser(\"My DGI\")\n",
        "\n",
        "parser.add_argument('--dataset',          type=str,           default=\"citeseer\",                help='data')\n",
        "parser.add_argument('--aug_type',         type=str,           default=\"node\",                help='aug type: mask or edge')\n",
        "parser.add_argument('--drop_percent',     type=float,         default=0.999,               help='drop percent')\n",
        "parser.add_argument('--seed',             type=int,           default=39,                help='seed')\n",
        "parser.add_argument('--gpu',              type=int,           default=0,                 help='gpu')\n",
        "parser.add_argument('--save_name',        type=str,           default='try.pkl_cora_node',                help='save ckpt name')\n",
        "\n",
        "args, unknown = parser.parse_known_args() ##\n",
        "\n",
        "print('-' * 100)\n",
        "print(args)\n",
        "print('-' * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9gjten3KPxg",
        "outputId": "a5ae95a7-ee11-469a-ac90-1d0d734359fb"
      },
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Namespace(dataset='citeseer', aug_type='node', drop_percent=0.999, seed=39, gpu=0, save_name='try.pkl_cora_node')\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = args.dataset\n",
        "aug_type = args.aug_type\n",
        "drop_percent = args.drop_percent\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu) \n",
        "seed = args.seed\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "kTY_xyfLo6C6"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training params\n",
        "\n",
        "batch_size = 1\n",
        "nb_epochs = 10000\n",
        "patience = 20\n",
        "lr = 0.001\n",
        "l2_coef = 0.0\n",
        "drop_prob = 0.0\n",
        "hid_units = 512\n",
        "sparse = True\n",
        "\n",
        "\n",
        "nonlinearity = 'prelu' # special name to separate parameters\n",
        "\n",
        "adj, features, labels, idx_train, idx_val, idx_test = process.load_data(dataset)\n",
        "features, _ = process.preprocess_features(features)\n",
        "\n",
        "nb_nodes = features.shape[0]  # node number\n",
        "ft_size = features.shape[1]   # node features dim\n",
        "nb_classes = labels.shape[1]  # classes = 6\n",
        "\n",
        "features = torch.FloatTensor(features[np.newaxis])\n",
        "\n",
        "\n",
        "'''\n",
        "------------------------------------------------------------\n",
        "edge node mask subgraph\n",
        "------------------------------------------------------------\n",
        "'''\n",
        "print(\"Begin Aug:[{}]\".format(args.aug_type))\n",
        "if args.aug_type == 'edge':\n",
        "\n",
        "    aug_features1 = features\n",
        "    aug_features2 = features\n",
        "\n",
        "    aug_adj1 = aug_random_edge(adj, drop_percent=drop_percent) # random drop edges\n",
        "    aug_adj2 = aug_random_edge(adj, drop_percent=drop_percent) # random drop edges\n",
        "    \n",
        "elif args.aug_type == 'node':\n",
        "    \n",
        "    aug_features1, aug_adj1 = aug_drop_node(features, adj, drop_percent=drop_percent)\n",
        "    aug_features2, aug_adj2 = aug_drop_node(features, adj, drop_percent=drop_percent)\n",
        "    \n",
        "elif args.aug_type == 'subgraph':\n",
        "    \n",
        "    aug_features1, aug_adj1 = aug_subgraph(features, adj, drop_percent=drop_percent)\n",
        "    aug_features2, aug_adj2 = aug_subgraph(features, adj, drop_percent=drop_percent)\n",
        "\n",
        "elif args.aug_type == 'mask':\n",
        "\n",
        "    aug_features1 = aug_random_mask(features,  drop_percent=drop_percent)\n",
        "    aug_features2 = aug_random_mask(features,  drop_percent=drop_percent)\n",
        "    \n",
        "    aug_adj1 = adj\n",
        "    aug_adj2 = adj\n",
        "\n",
        "else:\n",
        "    assert False\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "------------------------------------------------------------\n",
        "'''\n",
        "\n",
        "adj = process.normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "aug_adj1 = process.normalize_adj(aug_adj1 + sp.eye(aug_adj1.shape[0]))\n",
        "aug_adj2 = process.normalize_adj(aug_adj2 + sp.eye(aug_adj2.shape[0]))\n",
        "\n",
        "if sparse:\n",
        "    sp_adj = process.sparse_mx_to_torch_sparse_tensor(adj)\n",
        "    sp_aug_adj1 = process.sparse_mx_to_torch_sparse_tensor(aug_adj1)\n",
        "    sp_aug_adj2 = process.sparse_mx_to_torch_sparse_tensor(aug_adj2)\n",
        "\n",
        "else:\n",
        "    adj = (adj + sp.eye(adj.shape[0])).todense()\n",
        "    aug_adj1 = (aug_adj1 + sp.eye(aug_adj1.shape[0])).todense()\n",
        "    aug_adj2 = (aug_adj2 + sp.eye(aug_adj2.shape[0])).todense()\n",
        "\n",
        "\n",
        "'''\n",
        "------------------------------------------------------------\n",
        "mask\n",
        "------------------------------------------------------------\n",
        "'''\n",
        "\n",
        "'''\n",
        "------------------------------------------------------------\n",
        "'''\n",
        "if not sparse:\n",
        "    adj = torch.FloatTensor(adj[np.newaxis])\n",
        "    aug_adj1 = torch.FloatTensor(aug_adj1[np.newaxis])\n",
        "    aug_adj2 = torch.FloatTensor(aug_adj2[np.newaxis])\n",
        "\n",
        "\n",
        "labels = torch.FloatTensor(labels[np.newaxis])\n",
        "idx_train = torch.LongTensor(idx_train)\n",
        "idx_val = torch.LongTensor(idx_val)\n",
        "idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "model = DGI(ft_size, hid_units, nonlinearity)\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_coef)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('Using CUDA')\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    aug_features1 = aug_features1.cuda()\n",
        "    aug_features2 = aug_features2.cuda()\n",
        "    if sparse:\n",
        "        sp_adj = sp_adj.cuda()\n",
        "        sp_aug_adj1 = sp_aug_adj1.cuda()\n",
        "        sp_aug_adj2 = sp_aug_adj2.cuda()\n",
        "    else:\n",
        "        adj = adj.cuda()\n",
        "        aug_adj1 = aug_adj1.cuda()\n",
        "        aug_adj2 = aug_adj2.cuda()\n",
        "\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "\n",
        "b_xent = nn.BCEWithLogitsLoss()\n",
        "xent = nn.CrossEntropyLoss()\n",
        "cnt_wait = 0\n",
        "best = 1e9\n",
        "best_t = 0\n",
        "\n",
        "loss_list=[]\n",
        "for epoch in range(nb_epochs):\n",
        "\n",
        "    model.train()\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    idx = np.random.permutation(nb_nodes) #negative sampling\n",
        "    shuf_fts = features[:, idx, :] #negative sampling\n",
        "\n",
        "    lbl_1 = torch.ones(batch_size, nb_nodes) \n",
        "    lbl_2 = torch.zeros(batch_size, nb_nodes)\n",
        "    lbl = torch.cat((lbl_1, lbl_2), 1)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        shuf_fts = shuf_fts.cuda()\n",
        "        lbl = lbl.cuda()\n",
        "    \n",
        "    logits = model(features, shuf_fts, aug_features1, aug_features2,\n",
        "                   sp_adj if sparse else adj, \n",
        "                   sp_aug_adj1 if sparse else aug_adj1,\n",
        "                   sp_aug_adj2 if sparse else aug_adj2,  \n",
        "                   sparse, None, None, None, aug_type=aug_type) \n",
        "\n",
        "    loss = b_xent(logits, lbl)\n",
        "    loss_list.append(loss.item())\n",
        "    print('Loss:[{:.4f}]'.format(loss.item()))\n",
        "\n",
        "    if loss < best:\n",
        "        best = loss\n",
        "        best_t = epoch\n",
        "        cnt_wait = 0\n",
        "        torch.save(model.state_dict(), args.save_name)\n",
        "    else:\n",
        "        cnt_wait += 1\n",
        "\n",
        "    if cnt_wait == patience:\n",
        "        print('Early stopping!')\n",
        "        break\n",
        "\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "print('Loading {}th epoch'.format(best_t))\n",
        "model.load_state_dict(torch.load(args.save_name))\n",
        "\n",
        "embeds, _ = model.embed(features, sp_adj if sparse else adj, sparse, None)\n",
        "train_embs = embeds[0, idx_train]\n",
        "val_embs = embeds[0, idx_val]\n",
        "test_embs = embeds[0, idx_test]\n",
        "\n",
        "train_lbls = torch.argmax(labels[0, idx_train], dim=1)\n",
        "val_lbls = torch.argmax(labels[0, idx_val], dim=1)\n",
        "test_lbls = torch.argmax(labels[0, idx_test], dim=1)\n",
        "\n",
        "tot = torch.zeros(1)\n",
        "tot = tot.cuda()\n",
        "\n",
        "accs = []\n",
        "\n",
        "acc_list=[]\n",
        "for _ in range(50):\n",
        "    log = LogReg(hid_units, nb_classes)\n",
        "    opt = torch.optim.Adam(log.parameters(), lr=0.01, weight_decay=0.0)\n",
        "    log.cuda()\n",
        "\n",
        "    pat_steps = 0\n",
        "    best_acc = torch.zeros(1)\n",
        "    best_acc = best_acc.cuda()\n",
        "    for _ in range(100):\n",
        "        log.train()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        logits = log(train_embs)\n",
        "        loss = xent(logits, train_lbls)\n",
        "        \n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "    logits = log(test_embs)\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    acc = torch.sum(preds == test_lbls).float() / test_lbls.shape[0]\n",
        "    accs.append(acc * 100)\n",
        "    print('acc:[{:.4f}]'.format(acc))\n",
        "    acc_list.append(acc.item())\n",
        "    tot += acc\n",
        "    \n",
        "\n",
        "print('-' * 100)\n",
        "print('Average accuracy:[{:.4f}]'.format(tot.item() / 50))\n",
        "accs = torch.stack(accs)\n",
        "print('Mean:[{:.4f}]'.format(accs.mean().item()))\n",
        "print('Std :[{:.4f}]'.format(accs.std().item()))\n",
        "print('-' * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ss0zc8xb7Zle",
        "outputId": "944c7b60-03ef-4eea-961a-61cca3e7faef"
      },
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-274-75ece5ad1c7d>:112: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
            "  objects.append(pkl.load(f, encoding='latin1'))\n",
            "<ipython-input-274-75ece5ad1c7d>:183: RuntimeWarning: divide by zero encountered in power\n",
            "  r_inv = np.power(rowsum, -1).flatten()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin Aug:[node]\n",
            "Using CUDA\n",
            "Loss:[0.6932]\n",
            "Loss:[0.6928]\n",
            "Loss:[0.6889]\n",
            "Loss:[0.6859]\n",
            "Loss:[0.6773]\n",
            "Loss:[0.6781]\n",
            "Loss:[0.6649]\n",
            "Loss:[0.6638]\n",
            "Loss:[0.6488]\n",
            "Loss:[0.6429]\n",
            "Loss:[0.6265]\n",
            "Loss:[0.6184]\n",
            "Loss:[0.5996]\n",
            "Loss:[0.5914]\n",
            "Loss:[0.5682]\n",
            "Loss:[0.5590]\n",
            "Loss:[0.5347]\n",
            "Loss:[0.5222]\n",
            "Loss:[0.5064]\n",
            "Loss:[0.4794]\n",
            "Loss:[0.4719]\n",
            "Loss:[0.4512]\n",
            "Loss:[0.4252]\n",
            "Loss:[0.4224]\n",
            "Loss:[0.4013]\n",
            "Loss:[0.3743]\n",
            "Loss:[0.3704]\n",
            "Loss:[0.3614]\n",
            "Loss:[0.3329]\n",
            "Loss:[0.3225]\n",
            "Loss:[0.3160]\n",
            "Loss:[0.2911]\n",
            "Loss:[0.2918]\n",
            "Loss:[0.2893]\n",
            "Loss:[0.2630]\n",
            "Loss:[0.2620]\n",
            "Loss:[0.2586]\n",
            "Loss:[0.2485]\n",
            "Loss:[0.2320]\n",
            "Loss:[0.2378]\n",
            "Loss:[0.2205]\n",
            "Loss:[0.2125]\n",
            "Loss:[0.2227]\n",
            "Loss:[0.2062]\n",
            "Loss:[0.1975]\n",
            "Loss:[0.2001]\n",
            "Loss:[0.1825]\n",
            "Loss:[0.1803]\n",
            "Loss:[0.1821]\n",
            "Loss:[0.1685]\n",
            "Loss:[0.1659]\n",
            "Loss:[0.1663]\n",
            "Loss:[0.1596]\n",
            "Loss:[0.1671]\n",
            "Loss:[0.1548]\n",
            "Loss:[0.1573]\n",
            "Loss:[0.1528]\n",
            "Loss:[0.1425]\n",
            "Loss:[0.1468]\n",
            "Loss:[0.1514]\n",
            "Loss:[0.1381]\n",
            "Loss:[0.1431]\n",
            "Loss:[0.1308]\n",
            "Loss:[0.1375]\n",
            "Loss:[0.1376]\n",
            "Loss:[0.1308]\n",
            "Loss:[0.1277]\n",
            "Loss:[0.1359]\n",
            "Loss:[0.1306]\n",
            "Loss:[0.1302]\n",
            "Loss:[0.1302]\n",
            "Loss:[0.1223]\n",
            "Loss:[0.1199]\n",
            "Loss:[0.1194]\n",
            "Loss:[0.1251]\n",
            "Loss:[0.1195]\n",
            "Loss:[0.1125]\n",
            "Loss:[0.1170]\n",
            "Loss:[0.1189]\n",
            "Loss:[0.1142]\n",
            "Loss:[0.1204]\n",
            "Loss:[0.1153]\n",
            "Loss:[0.1207]\n",
            "Loss:[0.1133]\n",
            "Loss:[0.1109]\n",
            "Loss:[0.1114]\n",
            "Loss:[0.1141]\n",
            "Loss:[0.1179]\n",
            "Loss:[0.1064]\n",
            "Loss:[0.1129]\n",
            "Loss:[0.1130]\n",
            "Loss:[0.1127]\n",
            "Loss:[0.1072]\n",
            "Loss:[0.1164]\n",
            "Loss:[0.1047]\n",
            "Loss:[0.1068]\n",
            "Loss:[0.1105]\n",
            "Loss:[0.1049]\n",
            "Loss:[0.1042]\n",
            "Loss:[0.1128]\n",
            "Loss:[0.1045]\n",
            "Loss:[0.1135]\n",
            "Loss:[0.1074]\n",
            "Loss:[0.1109]\n",
            "Loss:[0.1063]\n",
            "Loss:[0.1015]\n",
            "Loss:[0.1020]\n",
            "Loss:[0.1081]\n",
            "Loss:[0.0987]\n",
            "Loss:[0.0983]\n",
            "Loss:[0.1062]\n",
            "Loss:[0.1047]\n",
            "Loss:[0.1061]\n",
            "Loss:[0.1003]\n",
            "Loss:[0.1045]\n",
            "Loss:[0.0960]\n",
            "Loss:[0.1000]\n",
            "Loss:[0.1015]\n",
            "Loss:[0.0981]\n",
            "Loss:[0.1033]\n",
            "Loss:[0.0981]\n",
            "Loss:[0.1020]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-279-ccc6220fe60e>\u001b[0m in \u001b[0;36m<cell line: 131>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aug_adj1.shape"
      ],
      "metadata": {
        "id": "Nq5S5NvW3ZEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_adj2.shape"
      ],
      "metadata": {
        "id": "sUsth5hW3bdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_features1.shape"
      ],
      "metadata": {
        "id": "aCOOjjXE3i1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_features2.shape"
      ],
      "metadata": {
        "id": "88uKvLsi3lU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj.shape"
      ],
      "metadata": {
        "id": "IAPh87wr4UOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the loss and accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_list)\n",
        "plt.legend(['Citeseer Dataset : subgraph'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('unsupervised loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a8J8kD-W8OZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the loss and accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(acc_list)\n",
        "avg_acc = accs.mean().item()/100\n",
        "plt.hlines(accs.mean().item()/100, 0, 50, color='orange', linestyle='--', linewidth=2)\n",
        "plt.legend(['Cora Dataset : node', f'Average accuracy = {np.round(avg_acc, 4)}'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Accuracy (f1-micro)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TrB_EykHJuty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl"
      ],
      "metadata": {
        "id": "TRx7DIYQyDpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "from dgl import DGLGraph\n",
        "\n",
        "\n",
        "from dgl.data import CitationGraphDataset\n",
        "from dgl.data import CoraGraphDataset"
      ],
      "metadata": {
        "id": "XboBsEcix7vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "citeseer = CitationGraphDataset('citeseer')"
      ],
      "metadata": {
        "id": "Pt-kZhK7zlps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cora = CoraGraphDataset('cora')"
      ],
      "metadata": {
        "id": "leJK3Z03zmr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CoraGraphDataset()\n",
        "g = dataset[0]\n",
        "num_class = dataset.num_classes\n",
        "\n",
        "# get node feature\n",
        "feat = g.ndata['feat']\n",
        "\n",
        "# get data split\n",
        "train_mask = g.ndata['train_mask']\n",
        "val_mask = g.ndata['val_mask']\n",
        "test_mask = g.ndata['test_mask']\n",
        "\n",
        "# get labels\n",
        "label = g.ndata['label']"
      ],
      "metadata": {
        "id": "VlG40Ae3CjxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HKaax_oQCz4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mask.shape"
      ],
      "metadata": {
        "id": "vPtnG-DDC2yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_mask.shape"
      ],
      "metadata": {
        "id": "iaiYbEVgC4KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label"
      ],
      "metadata": {
        "id": "Q93CujklC8jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj"
      ],
      "metadata": {
        "id": "OZWPZdiWV0JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"{}\".format(adj))"
      ],
      "metadata": {
        "id": "03Z4YuHuV78I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix, find"
      ],
      "metadata": {
        "id": "1k2S1ffWX9rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(find(adj)[0])"
      ],
      "metadata": {
        "id": "1TLzIRhdXl38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(find(adj)[1])"
      ],
      "metadata": {
        "id": "uUzPcQUDYH6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find(adj)[2]"
      ],
      "metadata": {
        "id": "UM2h28mhYNTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(find(adj)[2])"
      ],
      "metadata": {
        "id": "lueXa5NZYJ6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_D_array = adj.todense()\n",
        "matrix_D_array = np.nan_to_num(matrix_D_array) \n",
        "print(matrix_D_array)"
      ],
      "metadata": {
        "id": "1kQrIDWwebFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(matrix_D_array)"
      ],
      "metadata": {
        "id": "_HLqVKZGgY_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "1DGUEY5mgdSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# L 에 대한 eigenvalue, eigenvector 계산 \n",
        "eigen_value , eigen_vector = np.linalg.eig( matrix_D_array )\n",
        "\n",
        "# eigen value 내림차순 정렬 \n",
        "order = np.absolute(eigen_value).argsort()[::-1] \n",
        "\n",
        "# 정렬 순서에 따라 재정렬 \n",
        "eigen_value = eigen_value[order] \n",
        "eigen_vector = eigen_vector[:,order] \n",
        "\n",
        "# 첫번째 eigen value 에 대한 eigenvector 추출 및 비중확인 \n",
        "r = eigen_vector[:,0]  # 0번째 열 \n",
        "value = 100*np.real(r/np.sum(r)) ## np.real : 복소수 인수의 실수부를 반환\n",
        "print(value) #pagerank"
      ],
      "metadata": {
        "id": "qVmNClFBfF4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value.tolist().index(max(value))"
      ],
      "metadata": {
        "id": "8nhWzN9tiO_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value.index(max(value))"
      ],
      "metadata": {
        "id": "7uuZ_JVxht89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.arange(0,value.shape[0])"
      ],
      "metadata": {
        "id": "neTph1hmg6es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_adv_pagernak = pd.DataFrame(value, columns = ['adv_pagerank',], index = np.arange(0,value.shape[0]))\n",
        "df_adv_pagernak"
      ],
      "metadata": {
        "id": "OD6U09o8gr0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_adv_pagernak.sort_values('adv_pagerank', ascending=False)"
      ],
      "metadata": {
        "id": "gIA_ZFJFhKBb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}